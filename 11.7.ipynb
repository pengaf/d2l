{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import d2l\n",
    "\n",
    "class PositionWiseFFN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dense2 = torch.nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dense2(self.relu(self.dense1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3742, -0.0382,  0.5203,  0.3202, -0.1715,  0.6132,  0.1372, -0.8487],\n",
       "        [ 0.3742, -0.0382,  0.5203,  0.3202, -0.1715,  0.6132,  0.1372, -0.8487],\n",
       "        [ 0.3742, -0.0382,  0.5203,  0.3202, -0.1715,  0.6132,  0.1372, -0.8487]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = PositionWiseFFN(4,4,8)\n",
    "ffn.eval()\n",
    "ffn(torch.ones((2,3,4)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0000,  1.0000],\n",
       "         [-1.0000,  1.0000]], grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[-1.0000, -1.0000],\n",
       "         [ 1.0000,  1.0000]], grad_fn=<NativeBatchNormBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln = torch.nn.LayerNorm(2)\n",
    "bn = torch.nn.BatchNorm1d(2)\n",
    "x = torch.tensor([[1,2],[2,3]],dtype=torch.float32)\n",
    "ln(x),bn(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(torch.nn.Module):\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.ln = torch.nn.LayerNorm(normalized_shape)\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        return self.ln(self.dropout(y)+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_norm = AddNorm([3,4],0.5)\n",
    "add_norm.eval()\n",
    "add_norm(torch.ones(2,3,4),torch.ones(2,3,4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, fnn_hidden_size, dropout, bias = False):\n",
    "        super().__init__()\n",
    "        self.attention = d2l.MultiHeadAttention(hidden_size, num_heads, dropout, bias)\n",
    "        self.add_norm1 = AddNorm(hidden_size, dropout)\n",
    "        self.fnn = PositionWiseFFN(hidden_size, fnn_hidden_size, hidden_size)\n",
    "        self.add_norm2 = AddNorm(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, x, valid_lens):\n",
    "        y = self.add_norm1(x, self.attention(x,x,x, valid_lens))\n",
    "        return self.add_norm2(y, self.fnn(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((2, 100, 24))\n",
    "valid_lens = torch.tensor([3, 2])\n",
    "encoder_blk = TransformerEncoderBlock(24, 8, 48, 0.5)\n",
    "encoder_blk.eval()\n",
    "encoder_blk(x, valid_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_heads, fnn_hidden_size, num_blocks, dropout, bias = False) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(hidden_size, dropout)\n",
    "        self.blocks = torch.nn.Sequential()\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.add_module(\"block\"+str(i), TransformerEncoderBlock(hidden_size, num_heads, fnn_hidden_size,dropout,bias))\n",
    "        \n",
    "    def forward(self, x, valid_lens=None):\n",
    "        x = self.pos_encoding(self.embedding(x)*math.sqrt(self.hidden_size))\n",
    "        self.attention_weights = [None]*len(self.blocks)\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x, valid_lens)\n",
    "            self.attention_weights[i] = block.attention.attention.attention_weights\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(200, 24, 8, 48, 2, 0.5)\n",
    "encoder(torch.ones((2,100),dtype=torch.long)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, fnn_hidden_size, dropout, i) -> None:\n",
    "        super().__init__()\n",
    "        self.i=i\n",
    "        self.attention1 = d2l.MultiHeadAttention(hidden_size,num_heads,dropout)\n",
    "        self.add_norm1 = AddNorm(hidden_size, dropout)\n",
    "        self.attention2 = d2l.MultiHeadAttention(hidden_size,num_heads,dropout)\n",
    "        self.add_norm2 = AddNorm(hidden_size, dropout)\n",
    "        self.fnn = PositionWiseFFN(hidden_size, fnn_hidden_size, hidden_size)\n",
    "        self.add_norm3 = AddNorm(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = x\n",
    "        else:\n",
    "            key_values = torch.cat((state[2][self.i],x),dim=1)\n",
    "        state[2][self.i] = key_values\n",
    "\n",
    "        if self.training:\n",
    "            batch_size, num_steps,_ = x.shape\n",
    "            dec_valid_lens = torch.arange(1, num_steps+1,device=x.device).repeat(batch_size, 1)\n",
    "        else:\n",
    "            dec_valid_lens = None\n",
    "        \n",
    "        x2 = self.attention1(x,key_values,key_values,dec_valid_lens)\n",
    "        y = self.add_norm1(x,x2)\n",
    "        y2 = self.attention2(y,enc_outputs,enc_outputs,enc_valid_lens)\n",
    "        z = self.add_norm2(y,y2)\n",
    "        return self.add_norm3(z, self.fnn(z)), state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_heads, fnn_hidden_size, num_blocks, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_blocks = num_blocks\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(hidden_size, dropout)\n",
    "        self.blocks = torch.nn.Sequential()\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.add_module(\"block\"+str(i), TransformerDecoderBlock(hidden_size,num_heads,fnn_hidden_size,dropout,i))\n",
    "        self.dense = torch.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens):\n",
    "        return [enc_outputs, enc_valid_lens, [None]*self.num_blocks]\n",
    "    \n",
    "    def forward(self, x, state):\n",
    "        x = self.pos_encoding(self.embedding(x)*math.sqrt(self.hidden_size))\n",
    "        self.attention_weights=[[None]*self.num_blocks for _ in range(2)]\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, state = block(x, state)\n",
    "            self.attention_weights[0][i] = block.attention1.attention.attention_weights\n",
    "            self.attention_weights[1][i] = block.attention2.attention.attention_weights\n",
    "        return self.dense(x), state\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
