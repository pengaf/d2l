{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import d2l\n",
    "import math\n",
    "\n",
    "class PositionWiseFFN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dense2 = torch.nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dense2(self.relu(self.dense1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0455,  0.4076,  0.5875, -0.3784, -0.0044,  0.2528,  0.4329,  0.3892],\n",
       "        [ 0.0455,  0.4076,  0.5875, -0.3784, -0.0044,  0.2528,  0.4329,  0.3892],\n",
       "        [ 0.0455,  0.4076,  0.5875, -0.3784, -0.0044,  0.2528,  0.4329,  0.3892]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = PositionWiseFFN(4,4,8)\n",
    "ffn.eval()\n",
    "ffn(torch.ones((2,3,4)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0000,  1.0000],\n",
       "         [-1.0000,  1.0000]], grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[-1.0000, -1.0000],\n",
       "         [ 1.0000,  1.0000]], grad_fn=<NativeBatchNormBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln = torch.nn.LayerNorm(2)\n",
    "bn = torch.nn.BatchNorm1d(2)\n",
    "x = torch.tensor([[1,2],[2,3]],dtype=torch.float32)\n",
    "ln(x),bn(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(torch.nn.Module):\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.ln = torch.nn.LayerNorm(normalized_shape)\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        return self.ln(self.dropout(y)+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_norm = AddNorm([3,4],0.5)\n",
    "add_norm.eval()\n",
    "add_norm(torch.ones(2,3,4),torch.ones(2,3,4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, fnn_hidden_size, dropout, bias = False):\n",
    "        super().__init__()\n",
    "        self.attention = d2l.MultiHeadAttention(hidden_size, num_heads, dropout, bias)\n",
    "        self.add_norm1 = AddNorm(hidden_size, dropout)\n",
    "        self.fnn = PositionWiseFFN(hidden_size, fnn_hidden_size, hidden_size)\n",
    "        self.add_norm1 = AddNorm(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, x, valid_lens):\n",
    "        y = self.add_norm1(x, self.attention(x,x,x, valid_lens))\n",
    "        return self.add_norm2(y, self.fnn(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TransformerEncoderBlock.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m encoder_blk \u001b[38;5;241m=\u001b[39m TransformerEncoderBlock(\u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m8\u001b[39m,  \u001b[38;5;241m48\u001b[39m, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m      7\u001b[0m encoder_blk\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m----> 8\u001b[0m \u001b[43mencoder_blk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_lens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: TransformerEncoderBlock.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "#import importlib\n",
    "#importlib.reload(d2l)\n",
    "\n",
    "x = torch.ones((2, 100, 24))\n",
    "valid_lens = torch.tensor([3, 2])\n",
    "encoder_blk = TransformerEncoderBlock(24, 8, 48, 0.5)\n",
    "encoder_blk.eval()\n",
    "encoder_blk(x, valid_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_heads, fnn_hidden_size, num_blocks, dropout, bias = False) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(hidden_size, dropout)\n",
    "        self.blocks = torch.nn.Sequential()\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.add_module(\"block\"+str(i), TransformerEncoderBlock(hidden_size, num_heads, fnn_hidden_size,dropout,bias))\n",
    "        \n",
    "    def forward(self, x, valid_lens):\n",
    "        x = self.pos_encoding(self.embedding(x)*math.sqrt(self.hidden_size))\n",
    "        self.attention_weights = [None]*len(self.blocks)\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x, valid_lens)\n",
    "            self.attention_weights[i] = block.attention.attention.attention_weights\n",
    "        return x\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
